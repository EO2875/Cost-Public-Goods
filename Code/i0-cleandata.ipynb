{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as a dll could not be loaded.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresDllLoad'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from utils import data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Etienne\\AppData\\Local\\Temp\\ipykernel_11036\\810743668.py:2: DtypeWarning: Columns (364,365,366,394,410,821,831,832,930,1235,1252,1481,3063,3282,3283,3351,3353,3357,3358,3360,3361,3362) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  vdem_13 = pd.read_csv(os.path.join(data_dir, 'V-Dem-v13-CY-Full+Others.csv'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_name</th>\n",
       "      <th>year</th>\n",
       "      <th>SP.URB.TOTL.IN.ZS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2022</td>\n",
       "      <td>26.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2021</td>\n",
       "      <td>26.314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2020</td>\n",
       "      <td>26.026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2019</td>\n",
       "      <td>25.754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2018</td>\n",
       "      <td>25.495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13540</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>1964</td>\n",
       "      <td>14.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13541</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>1963</td>\n",
       "      <td>13.578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13542</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>1962</td>\n",
       "      <td>13.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13543</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>1961</td>\n",
       "      <td>12.821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13544</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>1960</td>\n",
       "      <td>12.608</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13545 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      country_name  year  SP.URB.TOTL.IN.ZS\n",
       "0      Afghanistan  2022             26.616\n",
       "1      Afghanistan  2021             26.314\n",
       "2      Afghanistan  2020             26.026\n",
       "3      Afghanistan  2019             25.754\n",
       "4      Afghanistan  2018             25.495\n",
       "...            ...   ...                ...\n",
       "13540     Zimbabwe  1964             14.092\n",
       "13541     Zimbabwe  1963             13.578\n",
       "13542     Zimbabwe  1962             13.082\n",
       "13543     Zimbabwe  1961             12.821\n",
       "13544     Zimbabwe  1960             12.608\n",
       "\n",
       "[13545 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wb_urbpop = pd.read_csv(os.path.join(data_dir, 'WB_UrbanPopPer.csv'))\n",
    "vdem_13 = pd.read_csv(os.path.join(data_dir, 'V-Dem-v13-CY-Full+Others.csv'))\n",
    "\n",
    "wb_urbpop.drop(columns=['country_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['American Samoa', 'Andorra', 'Antigua and Barbuda', 'Aruba', 'Bahamas, The', 'Belize', 'Bermuda', 'British Virgin Islands', 'Brunei Darussalam', 'Cabo Verde', 'Cayman Islands', 'Channel Islands', 'Congo, Rep.', \"Cote d'Ivoire\", 'Curacao', 'Dominica', 'Faroe Islands', 'French Polynesia', 'Gibraltar', 'Greenland', 'Grenada', 'Guam', 'Isle of Man', 'Kiribati', 'Liechtenstein', 'Macao SAR, China', 'Marshall Islands', 'Micronesia, Fed. Sts.', 'Monaco', 'Nauru', 'New Caledonia', 'Northern Mariana Islands', 'Palau', 'Puerto Rico', 'Samoa', 'San Marino', 'Sint Maarten (Dutch part)', 'St. Kitts and Nevis', 'St. Lucia', 'St. Vincent and the Grenadines', 'Tonga', 'Turks and Caicos Islands', 'Tuvalu', 'Virgin Islands (U.S.)']\n",
    "['Baden', 'Bavaria', 'Brunswick', 'Cape Verde', 'German Democratic Republic', 'Hamburg', 'Hanover', 'Hesse-Darmstadt', 'Hesse-Kassel', 'Ivory Coast', 'Kosovo', 'Mecklenburg Schwerin', 'Modena', 'Nassau', 'Oldenburg', 'Palestine/British Mandate', 'Palestine/Gaza', 'Papal States', 'Parma', 'Piedmont-Sardinia', 'Republic of Vietnam', 'Republic of the Congo', 'Saxe-Weimar-Eisenach', 'Saxony', 'Somaliland', 'South Yemen', 'Taiwan', 'Tuscany', 'Two Sicilies', 'Würtemberg', 'Zanzibar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in df1 but not in df2: 43\n",
      "New size of DF1 10836\n",
      "Found in df2 but not in df1: 30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wb_to_vdem = {\n",
    "    'Gambia, The':'The Gambia',\n",
    "    'United States':'United States of America',\n",
    "    'Venezuela, RB':'Venezuela',\n",
    "    'Ivory Coast':\"Cote d'Ivoire\",\n",
    "    'Lao PDR':'Laos',\n",
    "    'Hong Kong SAR, China':'Hong Kong',\n",
    "    'Yemen, Rep.':'Yemen',\n",
    "    'Turkiye':'Turkey',\n",
    "    'Syrian Arab Republic':'Syria',\n",
    "    'Czech Republic':'Czechia',\n",
    "    'Russian Federation':'Russia',\n",
    "    'Myanmar': 'Burma/Myanmar',\n",
    "    'Congo, Rep.': 'Republic of the Congo',\n",
    "    'Congo, Dem. Rep.': 'Democratic Republic of the Congo',\n",
    "    'Egypt, Arab Rep.': 'Egypt',\n",
    "    'Iran, Islamic Rep.': 'Iran',\n",
    "    \"Korea, Dem. People's Rep.\": 'North Korea',\n",
    "    'Korea, Rep.': 'South Korea',\n",
    "    'Kyrgyz Republic': 'Kyrgyzstan',\n",
    "    'Slovak Republic': 'Slovakia',\n",
    "    'Viet Nam':'Vietnam',\n",
    "    'West Bank and Gaza': 'Palestine/West Bank',\n",
    "}\n",
    "\n",
    "def compare_values(df1: pd.DataFrame, df2: pd.DataFrame, col: str):\n",
    "    df1_c = set(df1[col])\n",
    "    df2_c = set(df2[col])\n",
    "\n",
    "    d12 = sorted(df1_c - df2_c)\n",
    "    print('Found in df1 but not in df2:', len(df1_c - df2_c))\n",
    "    # print(sorted(df1_c - df2_c))\n",
    "    print('New size of DF1', len(df1[df1[col].isin(df2_c)]))\n",
    "\n",
    "    d21 = sorted(df2_c - df1_c)\n",
    "    print('Found in df2 but not in df1:', len(df2_c - df1_c))\n",
    "    # print(sorted(df2_c - df1_c))\n",
    "\n",
    "    # ld = len(d12) - len(d21)\n",
    "    # # Extend the shorter list with None values\n",
    "    # if ld > 0:\n",
    "    #     d21.extend([None] * ld)\n",
    "    # elif ld < 0:\n",
    "    #     d12.extend([None] * abs(ld))\n",
    "\n",
    "    # print(pd.DataFrame({'df1': d12, 'df2': d21}))\n",
    "\n",
    "\n",
    "wb_urbpop.drop(columns=['country_id'])\n",
    "\n",
    "wb_urbpop['country_name'] = wb_urbpop['country_name'].replace(wb_to_vdem)\n",
    "\n",
    "compare_values(wb_urbpop, vdem_13, 'country_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wb_urbpop = wb_urbpop.drop(columns=['country_id'])\n",
    "wb_urbpop = wb_urbpop.rename(columns={'SP.URB.TOTL.IN.ZS': 'wb_urbpop'})\n",
    "\n",
    "wb_urbpop['country_name'] = wb_urbpop['country_name'].replace(wb_to_vdem)\n",
    "\n",
    "vdem_13_urbpop = pd.merge(vdem_13, wb_urbpop, on=['country_name', 'year'])\n",
    "\n",
    "vdem_13_urbpop.to_csv(os.path.join(data_dir, 'VDem_13_urbpop.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from utils import data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "epr_ed = pd.read_csv(os.path.join(data_dir, 'EPR-ED-2021.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgroups: list[pd.DataFrame] = []\n",
    "for r in range(1,4): # Religion\n",
    "    for l in range(1,4): # Language\n",
    "        for p in range(1,4): # Phenotype\n",
    "            group = epr_ed[[\n",
    "                'gwid', 'gwgroupid',\n",
    "                f'religion{r}', f'rel{r}_size', \n",
    "                f'language{l}', f'lang{l}_size', \n",
    "                f'phenotype{p}', f'pheno{p}_size'\n",
    "            ]]\n",
    "            group = group.dropna()\n",
    "            group = group.rename(columns={\n",
    "                f'religion{r}': 'rel', \n",
    "                f'rel{r}_size': 'rel_size', \n",
    "                f'language{l}': 'lang', \n",
    "                f'lang{l}_size': 'lang_size', \n",
    "                f'phenotype{p}': 'phenotype', \n",
    "                f'pheno{p}_size': 'pheno_size',\n",
    "            })\n",
    "            subgroups.append(group)\n",
    "\n",
    "expanded_ethn = pd.concat(subgroups)\n",
    "expanded_ethn.sort_values(['gwid', 'gwgroupid'], inplace=True)\n",
    "expanded_ethn.to_csv('Try1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils import data_dir, ldnd_db_fn\n",
    "\n",
    "lang_r_db = pd.read_csv(ldnd_db_fn)\n",
    "lang_r_db.set_index('l2id', inplace=True)\n",
    "\n",
    "epr_core = pd.read_csv(os.path.join(data_dir, 'EPR-Core-2021.csv'))\n",
    "epr_ed = pd.read_csv(os.path.join(data_dir, 'EPR-ED-2021.csv'), keep_default_na=False, na_values=['_'])\n",
    "epr_ed.set_index('gwgroupid', inplace=True)\n",
    "\n",
    "eth_langs = epr_ed['language1'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0861288550067874"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_r_db['ldnd'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerLevels:\n",
    "    def __init__(self, power_levels, exclude=()) -> None:\n",
    "        self.power_levels = power_levels\n",
    "        self.included_levels = [pl for pl in power_levels if pl not in exclude]\n",
    "\n",
    "        self.status_to_pl = {\n",
    "            **{ power_levels[i]: i for i in range(len(self.included_levels)) },\n",
    "            **{ pl: -1 for pl in exclude },\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def n_levels(self):\n",
    "        return max(self.status_to_pl.values()) - min(self.status_to_pl.values())\n",
    "\n",
    "    def diff(self, status1, status2):\n",
    "        \"\"\" Operationalize power difference. \"\"\"\n",
    "        n1 = self.status_to_pl[status1]\n",
    "        n2 = self.status_to_pl[status2]\n",
    "\n",
    "        return abs(n1 - n2) / self.n_levels\n",
    "\n",
    "power_levels = \"\"\"DISCRIMINATED\n",
    "POWERLESS\n",
    "JUNIOR PARTNER\n",
    "SENIOR PARTNER\n",
    "DOMINANT\n",
    "MONOPOLY\"\"\".split('\\n')\n",
    "\n",
    "unknown_statuses = { 'SELF-EXCLUSION', 'IRRELEVANT', 'STATE COLLAPSE', }\n",
    "extreme_statuses = { 'DISCRIMINATED', 'MONOPOLY', }\n",
    "\n",
    "excluded_powers = { *unknown_statuses, *extreme_statuses }\n",
    "\n",
    "# pl_all = PowerLevels(power_levels)\n",
    "\n",
    "pl_minus_unknown = PowerLevels(power_levels, unknown_statuses)\n",
    "pl_minus_unknown.status_to_pl['IRRELEVANT'] = pl_minus_unknown.status_to_pl['POWERLESS']\n",
    "\n",
    "# pl_minus_excluded = PowerLevels(power_levels, excluded_powers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cases:\n",
    "\n",
    "1. If there exists 1 ethnicity with a status = STATE COLLAPSE, then _all_\n",
    "    ethnicities have the same status (in the database, this holds true). \n",
    "    We assume that in this period, class distinction becomes blurry. We assume\n",
    "    power diff = 0\n",
    "2. If an ethnicity chooses SELF-EXCLUSION, then we assume this is because their \n",
    "    class-based differences are too great to be reconciled. Then power diff = 1\n",
    "    for all pairs except with itself. \n",
    "    One example is the Mayan ethnicity in Mexico, who live in autonomous\n",
    "    municipalities in Chiapas.\n",
    "3. Ethnicities that have an IRRELEVANT status are not easily classified. There\n",
    "    exists some possibilities as to why EPR classifies someone as IRRELEVANT:\n",
    "\n",
    "    3.1 There exists only 1 majority ethnicity, and therefore there is no\n",
    "        class struggle. Er = 1\n",
    "    3.2 Others. We assume IRRELEVANT equals POWERLESS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_uncat(group: pd.DataFrame):\n",
    "    group[['gwid', 'from', 'to']]\n",
    "    n_groups = n_cat_groups = len(group)\n",
    "    cat_size = group['size'].sum()\n",
    "\n",
    "    uncat_size = 1 - cat_size # Size of Uncategorized ethnicities\n",
    "    if uncat_size > 0.01:\n",
    "        n_groups += 1\n",
    "        # 'gwid', 'statename', 'from', 'to', 'group', 'groupid', 'gwgroupid', 'umbrella', 'size', 'status'\n",
    "        new_row_data = [\n",
    "            group['gwid'].iloc[0],\n",
    "            group['statename'].iloc[0],\n",
    "            group['from'].iloc[0],\n",
    "            group['to'].iloc[0],\n",
    "            'UNCATEGORIZED',\n",
    "            0, # groupid\n",
    "            int(f\"{group['gwid'].iloc[0]}00000\"), # gwgroupid\n",
    "            float('nan'), # concat is deprecating None\n",
    "            uncat_size,\n",
    "            'POWERLESS',\n",
    "            float('nan'), # concat is deprecating None\n",
    "        ]\n",
    "        new_row = pd.DataFrame([new_row_data], columns=group.columns)\n",
    "        group = pd.concat([group, new_row])\n",
    "\n",
    "    group['N_CAT_GROUPS'] = n_cat_groups # number of categorized groups\n",
    "    group['N_GROUPS'] = n_groups         # total number of groups\n",
    "\n",
    "    # exclude_cond = group['status'].isin(excluded_powers)\n",
    "    # included_size = group[~exclude_cond]['size'].sum()\n",
    "\n",
    "    # def get_pol_weight(row):\n",
    "    #     if row['status'] in excluded_powers:\n",
    "    #         return 0\n",
    "    #     else:\n",
    "    #         return row['size'] / included_size\n",
    "\n",
    "    # group['pol_weight'] = group.apply(get_pol_weight, axis=1)\n",
    "\n",
    "    return group\n",
    "\n",
    "epr_r = epr_core.groupby(['gwid', 'from', 'to'], as_index=False).apply(fill_uncat)\n",
    "epr_r['power_level'] = epr_r['status'].replace(pl_minus_unknown.status_to_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.1321200714867258\n",
      "STD: 0.15344689685197788\n",
      "> 15%: 174\n",
      "< 15%: 391\n",
      "Uncat / Cat 0.11521207177814029\n",
      "Average % /group 0.10142369618206533\n"
     ]
    }
   ],
   "source": [
    "uncategorized = epr_r[epr_r['groupid'] == 0]\n",
    "print('Mean:', uncategorized['size'].mean())\n",
    "print('STD:', uncategorized['size'].std())\n",
    "print('> 15%:', len(uncategorized[uncategorized['size'] > 0.15]))\n",
    "print('< 15%:', len(uncategorized[uncategorized['size'] < 0.15]))\n",
    "print('Uncat / Cat', len(uncategorized) / len(epr_r))\n",
    "ngroups = len(epr_r.groupby(['gwid', 'from', 'to']).nunique())\n",
    "assert (len(epr_core.groupby(['gwid', 'from', 'to']).nunique()) == ngroups), \"Group numbers changed\"\n",
    "print('Average % /group', uncategorized['size'].sum() / ngroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncategorized Peoples\n",
    "\n",
    "The countries with the highest percentage of uncategorized peoples\n",
    "are the Arabian Gulf countries: Qatar, UAE, Kwait, Bahrain have more than 50%\n",
    "uncategorized. \n",
    "The large majority of countries with a percentage of uncategorized poeples\n",
    "above 15% are African countries. African countries are very diverse and has\n",
    "a great divide between rural and urban populations. \n",
    "\n",
    "Other countries include:\n",
    "\n",
    "- Luxembourg 47%\n",
    "- Brazil (1946-1977) 46%\n",
    "- Singapore 37%\n",
    "- Indonesia 25%\n",
    "- Trinidad and Tobago 23%\n",
    "- Afghanistan 22%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "epr_r = pd.DataFrame(epr_r.to_numpy(), columns=epr_r.columns)\n",
    "epr_r['gwid'] = epr_r['gwid'].astype(int)\n",
    "epr_r['from'] = epr_r['from'].astype(int)\n",
    "epr_r['to'] = epr_r['to'].astype(int)\n",
    "epr_r.to_csv(os.path.join(data_dir, 'EPR-Core-R.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median Ling R 0.115292860676153\n",
      "Mean Ling R 0.16863569893307984\n"
     ]
    }
   ],
   "source": [
    "eth_langs = epr_ed['language1'].to_dict()\n",
    "eth_rels = epr_ed['religion1'].to_dict()\n",
    "eth_phens = epr_ed['phenotype1'].to_dict()\n",
    "\n",
    "print('Median Ling R', lang_r_db['ling_resem'].median())\n",
    "print('Mean Ling R', lang_r_db['ling_resem'].mean())\n",
    "\n",
    "lr_m = lang_r_db['ling_resem'].median()\n",
    "uncat_phen = 1 # 7 total, so P(X=x) = 1/7\n",
    "uncat_rel = 1\n",
    "uncat_pol = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Etienne\\AppData\\Local\\Temp\\ipykernel_5200\\2496538275.py:135: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  epr_r = epr_r.groupby(categories).apply(\n"
     ]
    }
   ],
   "source": [
    "from ldnd import get_ling_resem, NoLang, LDN\n",
    "\n",
    "country_resems = []\n",
    "\n",
    "def calc_lang_r(row1: pd.Series, row2: pd.Series, uncat_lang_r=lr_m):\n",
    "    try:\n",
    "        lang1 = eth_langs[row1['gwgroupid']]\n",
    "        lang2 = eth_langs[row2['gwgroupid']]\n",
    "    except KeyError:\n",
    "        return uncat_lang_r\n",
    "\n",
    "    try:\n",
    "        lr = get_ling_resem(lang1, lang2)\n",
    "    except NoLang:\n",
    "        return lr_m\n",
    "\n",
    "    if np.isnan(lr):\n",
    "        return lr_m\n",
    "\n",
    "    return lr\n",
    "\n",
    "def diff_count(iterable1, iterable2):\n",
    "    count = 0\n",
    "    for ele1, ele2 in zip(iterable1, iterable2):\n",
    "        if ele1 != ele2:\n",
    "            return count\n",
    "        count += 1\n",
    "    return count\n",
    "\n",
    "def calc_rel_r(row1: pd.Series, row2: pd.Series, uncat_rel=1):\n",
    "    try:\n",
    "        rel1: str = eth_rels[row1['gwgroupid']]\n",
    "        rel2: str = eth_rels[row2['gwgroupid']]\n",
    "    except KeyError:\n",
    "        return uncat_rel\n",
    "\n",
    "    if rel1 == rel2:\n",
    "        return 1\n",
    "\n",
    "    other_religions = ['ANI', 'ZOR', 'ATH',] # unclassified into systems\n",
    "    for r in other_religions:\n",
    "        if rel1.startswith(r) or rel2.startswith(r):\n",
    "            return 0\n",
    "\n",
    "    return diff_count(rel1, rel2) / max(len(rel1), len(rel2))\n",
    "\n",
    "def calc_phen_r(row1: pd.Series, row2: pd.Series, uncat_phen_r=1):\n",
    "    try:\n",
    "        phen1: str = eth_phens[row1['gwgroupid']]\n",
    "        phen2: str = eth_phens[row2['gwgroupid']]\n",
    "    except KeyError:\n",
    "        return 1\n",
    "    return int(phen1 == phen2)\n",
    "\n",
    "def calc_pol_r(row1: pd.Series, row2: pd.Series):\n",
    "    if row1['status'] == row2['status']:\n",
    "        return 1\n",
    "    if row1['status'] == 'SELF-EXCLUSION' or row2['status'] == 'SELF-EXCLUSION':\n",
    "        return 0\n",
    "\n",
    "    return 1 - pl_minus_unknown.diff(row1['status'], row2['status'])\n",
    "\n",
    "def calc_r(row1: pd.Series, row2: pd.Series, uncat_lang_r=lr_m, uncat_pol_r=uncat_pol):\n",
    "\n",
    "    if row1['gwgroupid'] == row2['gwgroupid']:\n",
    "        return 1, 1, 1, 1\n",
    "\n",
    "    has_uncat = row1['groupid'] == 0 or row2['groupid'] == 0\n",
    "    if has_uncat:\n",
    "        return uncat_lang_r, uncat_pol_r, uncat_rel, uncat_phen\n",
    "    \n",
    "    lr = calc_lang_r(row1, row2, uncat_lang_r)\n",
    "    pr = calc_pol_r(row1, row2)\n",
    "    rr = calc_rel_r(row1, row2)\n",
    "    gr = calc_phen_r(row1, row2)\n",
    "\n",
    "    return lr, pr, rr, gr\n",
    "\n",
    "def calc_E_r(min_uncat=0.05, uncat_r=0.4, irr_r=0.4, skip_mono=True):\n",
    "    \"\"\"Calculate Expected Resemblance\n",
    "    \"\"\"\n",
    "    def do_calc(group: pd.DataFrame):\n",
    "        thisgroup = list(group.loc[:, ['gwid', 'statename', 'from', 'to']].iloc[0])\n",
    "\n",
    "        cat_size = group[group['status'] != 'UNCATEGORIZED']['size'].sum()\n",
    "        uncat_size = 1 - cat_size\n",
    "\n",
    "        if group['N_CAT_GROUPS'].iloc[0] == 1 and cat_size > 0.97:\n",
    "            # Homogeneous group\n",
    "            country_resems.append([*thisgroup, 1, 1, 1, 1])\n",
    "            return group\n",
    "\n",
    "        if skip_mono and group['status'].str.contains('MONOPOLY').any():\n",
    "            Er = 1\n",
    "            country_resems.append([*thisgroup, Er])\n",
    "            return group\n",
    "\n",
    "        # exclude_cond = group['status'].isin(excluded_powers)\n",
    "        # included = group[~exclude_cond]\n",
    "        Elr, Epr, Err, Egr = 0, 0, 0, 0\n",
    "        lrs, prs, rrs, grs = [], [], [], []\n",
    "        sizes = []\n",
    "        rows = list(group.iterrows())\n",
    "        for r1 in rows:\n",
    "            for r2 in rows:\n",
    "                row1 = r1[1]\n",
    "                row2 = r2[1]\n",
    "\n",
    "                lr, pr, rr, gr = calc_r(row1, row2)\n",
    "\n",
    "                lrs.append(lr)\n",
    "                prs.append(pr)\n",
    "                rrs.append(rr)\n",
    "                grs.append(gr)\n",
    "\n",
    "                size = row1['size'] * row2['size']\n",
    "                sizes.append(size)\n",
    "\n",
    "                # Expected resemblance\n",
    "                Elr += size * lr\n",
    "                Epr += size * pr\n",
    "                Err += size * rr\n",
    "                Egr += size * gr\n",
    "\n",
    "        country_resems.append([*thisgroup, Elr, Epr, Err, Egr])\n",
    "        return group\n",
    "\n",
    "    return do_calc\n",
    "\n",
    "# epr_r['gwid', 'from']\n",
    "# sorted(set(ind), key=lambda x: x[0] * 10_000 + x[1])\n",
    "\n",
    "categories = pd.Categorical(epr_r[['gwid', 'from']].apply(tuple, axis=1))\n",
    "\n",
    "epr_r = epr_r.groupby(categories).apply(\n",
    "    calc_E_r(min_uncat=0.03, skip_mono=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_r = pd.DataFrame(country_resems, columns=['gwid', 'statename', 'from', 'to', 'Elr', 'Epr', 'Err', 'Egr'])\n",
    "\n",
    "settings_id = f'ul={lr_m:.2f}-up={uncat_pol:.2f}-ur={uncat_rel:.2f}-ug={uncat_phen:.2f}'\n",
    "c_r.to_csv(os.path.join(data_dir, f'Resemblance-{settings_id}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand Rows\n",
    "def expand_row(row: pd.Series):\n",
    "    columns = list(row.index)\n",
    "    columns.remove('from')\n",
    "    columns.remove('to')\n",
    "    years = range(row['from'], row['to'] + 1)\n",
    "    return pd.DataFrame({\n",
    "        'year': years,\n",
    "        **{\n",
    "            c: [row[c]] * len(years)\n",
    "            for c in columns\n",
    "        }\n",
    "    })\n",
    "# Expanded Column Resemblances\n",
    "c_r_exp = pd.concat(c_r.apply(expand_row, axis=1).tolist(), ignore_index=True)\n",
    "c_r_exp = c_r_exp.sort_values(by=['statename', 'year']).reset_index(drop=True)\n",
    "c_r_exp.to_csv(os.path.join(data_dir, f'Resemblance-Full-{settings_id}.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
